AutoGluon:
  version: latest

ZS_BAG_ftt_b0:
  extends: AutoGluon
  params:
    _leaderboard_test: true
    _save_artifacts:
    - leaderboard
    - zeroshot
    - model_failures
    ag_args_ensemble:
      ag.max_time_limit: 3600
    calibrate: false
    fit_weighted_ensemble: false
    hyperparameters:
      FT_TRANSFORMER:
      - ag_args:
          name_suffix: _c1
      - ag_args:
          name_suffix: _c2
        model.ft_transformer.adapter_output_feature: 192
        model.ft_transformer.d_token: 192
        model.ft_transformer.ffn_d_hidden: 192
        model.ft_transformer.n_blocks: 3
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
      - ag_args:
          name_suffix: _c3
        model.ft_transformer.adapter_output_feature: 192
        model.ft_transformer.d_token: 192
        model.ft_transformer.ffn_d_hidden: 192
        model.ft_transformer.n_blocks: 2
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
      - ag_args:
          name_suffix: _c4
        model.ft_transformer.adapter_output_feature: 192
        model.ft_transformer.d_token: 192
        model.ft_transformer.ffn_d_hidden: 192
        model.ft_transformer.n_blocks: 4
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
      - ag_args:
          name_suffix: _c5
        model.ft_transformer.adapter_output_feature: 256
        model.ft_transformer.d_token: 256
        model.ft_transformer.ffn_d_hidden: 256
        model.ft_transformer.n_blocks: 3
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
      - ag_args:
          name_suffix: _c6
        model.ft_transformer.adapter_output_feature: 192
        model.ft_transformer.d_token: 192
        model.ft_transformer.ffn_d_hidden: 288
        model.ft_transformer.n_blocks: 3
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
      - ag_args:
          name_suffix: _c7
        model.ft_transformer.adapter_output_feature: 192
        model.ft_transformer.d_token: 192
        model.ft_transformer.ffn_d_hidden: 384
        model.ft_transformer.n_blocks: 3
        optimization.learning_rate: 0.0001
        optimization.weight_decay: 1.0e-05
    num_bag_folds: 8
    num_bag_sets: 1
  version: latest
